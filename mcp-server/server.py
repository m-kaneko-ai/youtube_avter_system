#!/usr/bin/env python3
"""
Human-First AIã‚¹ã‚¿ãƒƒãƒ• MCPã‚µãƒ¼ãƒãƒ¼

é‡‘å­å¼ãƒ»äººé–“ä¸­å¿ƒAIå¢—å¹…ã‚·ã‚¹ãƒ†ãƒ ã®5äººã®AIã‚¹ã‚¿ãƒƒãƒ•ã‚’
Claude Desktop/Claude Codeã‹ã‚‰MCPãƒ„ãƒ¼ãƒ«ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹

ä½¿ç”¨æ–¹æ³•:
  Claude Desktop/Codeã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ å¾Œã€
  ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

  - sachiko_respond: ç§˜æ›¸ã‚µãƒã‚³ã«ã‚ˆã‚‹å¿œç­”
  - kenji_research: ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã‚±ãƒ³ã‚¸ã«ã‚ˆã‚‹èª¿æŸ»
  - yuta_create: ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ¦ã‚¦ã‚¿ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆ
  - makoto_check: å“è³ªç®¡ç†ãƒã‚³ãƒˆã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯
  - naomi_analyze: å­¦ç¿’ãƒŠã‚ªãƒŸã«ã‚ˆã‚‹åˆ†æ
"""

import json
import os
from datetime import datetime
from enum import Enum
from typing import Optional, List, Dict, Any
from pathlib import Path

from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv

# Central DBç’°å¢ƒè¨­å®šã‚’èª­ã¿è¾¼ã¿
env_path = Path.home() / ".bluelamp" / "central-db.env"
if env_path.exists():
    load_dotenv(env_path)


# =============================================================================
# MCPã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–
# =============================================================================

mcp = FastMCP("human-first-ai-staff")


# =============================================================================
# Central DBæ¥ç¶š
# =============================================================================

class CentralDBClient:
    """Central DBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆPostgreSQL + pgvector + OpenAI Embeddingï¼‰"""

    def __init__(self):
        self._pool = None
        self._openai = None
        self.database_url = os.getenv("DATABASE_URL")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

    def _get_pool(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
        if self._pool is None and self.database_url:
            try:
                import psycopg
                from psycopg_pool import ConnectionPool
                self._pool = ConnectionPool(self.database_url, min_size=1, max_size=5)
            except ImportError:
                # psycopg_poolãŒãªã„å ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«æ¥ç¶š
                pass
            except Exception as e:
                print(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return self._pool

    def _get_openai(self):
        """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
        if self._openai is None and self.openai_api_key:
            try:
                from openai import OpenAI
                self._openai = OpenAI(api_key=self.openai_api_key)
            except Exception as e:
                print(f"OpenAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return self._openai

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®Embeddingã‚’ç”Ÿæˆ"""
        client = self._get_openai()
        if not client:
            return None

        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text[:8000]  # 8000æ–‡å­—åˆ¶é™
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def search_knowledge(
        self,
        query: str,
        category: Optional[str] = None,
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """ãƒŠãƒ¬ãƒƒã‚¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆRAGï¼‰"""
        if not self.database_url:
            return []

        # ã‚¯ã‚¨ãƒªã®Embeddingç”Ÿæˆ
        query_embedding = self.generate_embedding(query)
        if not query_embedding:
            return []

        try:
            import psycopg

            with psycopg.connect(self.database_url) as conn:
                with conn.cursor() as cur:
                    if category:
                        sql = """
                            SELECT id, title, content, summary, category, subcategory, tags, source,
                                   1 - (embedding <=> %s::vector) as similarity
                            FROM knowledge_base
                            WHERE embedding IS NOT NULL AND category = %s
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """
                        cur.execute(sql, (json.dumps(query_embedding), category, json.dumps(query_embedding), limit))
                    else:
                        sql = """
                            SELECT id, title, content, summary, category, subcategory, tags, source,
                                   1 - (embedding <=> %s::vector) as similarity
                            FROM knowledge_base
                            WHERE embedding IS NOT NULL
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """
                        cur.execute(sql, (json.dumps(query_embedding), json.dumps(query_embedding), limit))

                    columns = [desc[0] for desc in cur.description]
                    results = []
                    for row in cur.fetchall():
                        result = dict(zip(columns, row))
                        # UUIDã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                        if 'id' in result:
                            result['id'] = str(result['id'])
                        # similarityã‚’å°æ•°ç‚¹4æ¡ã«
                        if 'similarity' in result:
                            result['similarity'] = round(float(result['similarity']), 4)
                        results.append(result)

                    return results

        except Exception as e:
            print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def get_categories(self) -> Dict[str, int]:
        """ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã¨ä»¶æ•°ã‚’å–å¾—"""
        if not self.database_url:
            return {}

        try:
            import psycopg

            with psycopg.connect(self.database_url) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT c.id, c.name, c.description, c.icon,
                               COALESCE(k.count, 0) as knowledge_count
                        FROM categories c
                        LEFT JOIN (
                            SELECT category, COUNT(*) as count
                            FROM knowledge_base
                            GROUP BY category
                        ) k ON c.id = k.category
                        ORDER BY c.name
                    """)

                    results = {}
                    for row in cur.fetchall():
                        results[row[0]] = {
                            "name": row[1],
                            "description": row[2],
                            "icon": row[3],
                            "count": row[4]
                        }
                    return results

        except Exception as e:
            print(f"ã‚«ãƒ†ã‚´ãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«Central DBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
central_db = CentralDBClient()


# =============================================================================
# å…±é€šå®šç¾©
# =============================================================================

class CustomerTier(str, Enum):
    """é¡§å®¢éšå±¤"""
    PREMIUM = "premium"
    STANDARD = "standard"
    ENTRY = "entry"
    LINE = "line"
    FREE = "free"


class EscalationLevel(str, Enum):
    """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«"""
    HIGH = "high"
    MID = "mid"
    LOW = "low"


# æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
EMOTION_KEYWORDS = {
    "high": ["æ­»ã«ãŸã„", "è‡ªæ®º", "è‡ªå‚·", "æ­»ã¬", "æ®º", "æ¶ˆãˆãŸã„"],
    "mid": ["è¾›ã„", "ã¤ã‚‰ã„", "æ‚²ã—ã„", "åŠ©ã‘ã¦", "é™ç•Œ", "é›¢å©š", "ç—…æ°—", "å€Ÿé‡‘"],
    "low": ["ä¸å®‰", "å¿ƒé…", "ãƒ¢ãƒ¤ãƒ¢ãƒ¤", "ã‚‚ã‚„ã‚‚ã‚„", "æ‚©ã¿", "è¿·ã„"],
}

# HSPé¿ã‘ã‚‹ã¹ãè¡¨ç¾
HSP_AVOID = ["ã™ãã«", "ä»Šã™ã", "çµ¶å¯¾", "å¿…ãš", "ã€œã—ãªã‘ã‚Œã°", "ã€œã™ã¹ã"]


def check_emotion(text: str) -> Optional[EscalationLevel]:
    """æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯"""
    for level, keywords in EMOTION_KEYWORDS.items():
        for kw in keywords:
            if kw in text:
                return EscalationLevel(level)
    return None


def add_ai_disclosure(content: str, agent_name: str) -> str:
    """AIé–‹ç¤ºãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ """
    header = f"AI{agent_name}ã§ã™\n\n"
    footer = "\n\n---\nã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯AIè‡ªå‹•è¿”ä¿¡ã§ã™\né‡‘å­ã¸ã®ç›´æ¥ç›¸è«‡ã¯ã„ã¤ã§ã‚‚ã©ã†ã"
    return header + content + footer


# =============================================================================
# ã‚µãƒã‚³ï¼ˆç§˜æ›¸ï¼‰
# =============================================================================

def _search_faq_knowledge(query: str) -> Optional[str]:
    """Central DBã‹ã‚‰FAQé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ¤œç´¢"""
    results = central_db.search_knowledge(
        query=query,
        category="questions",
        limit=3
    )
    if results:
        answers = []
        for r in results:
            title = r.get("title", "")
            content = r.get("content", "")[:200]
            if title:
                answers.append(f"ãƒ»{title}")
        return "\n".join(answers) if answers else None
    return None


@mcp.tool()
def sachiko_respond(
    message: str,
    customer_name: str = "ãŠå®¢æ§˜",
    tier: str = "free",
    use_knowledge: bool = True,
) -> str:
    """
    ç§˜æ›¸ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œã‚µãƒã‚³ã€ã«ã‚ˆã‚‹å¿œç­”

    FAQå¯¾å¿œã€ã‚¿ã‚¹ã‚¯ç®¡ç†ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«èª¿æ•´ãªã©ã‚’è¡Œã„ã¾ã™ã€‚
    Central DBã‹ã‚‰é–¢é€£ã™ã‚‹FAQãƒŠãƒ¬ãƒƒã‚¸ã‚’æ¤œç´¢ã—ã¦å›ç­”ã®å‚è€ƒã«ã—ã¾ã™ã€‚
    æ„Ÿæƒ…çš„ãªè³ªå•ã¯é‡‘å­ã•ã‚“ã¸ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã™ã€‚

    Args:
        message: ãŠå®¢æ§˜ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        customer_name: ãŠå®¢æ§˜ã®åå‰
        tier: é¡§å®¢éšå±¤ (premium/standard/entry/line/free)
        use_knowledge: Central DBã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ¤œç´¢ã™ã‚‹ã‹

    Returns:
        ã‚µãƒã‚³ã‹ã‚‰ã®å¿œç­”
    """
    # éšå±¤ãƒã‚§ãƒƒã‚¯
    if tier in ["premium", "standard"]:
        return f"ã€ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‘\n{tier}ã®ãŠå®¢æ§˜ã¸ã®å¯¾å¿œã¯é‡‘å­ã•ã‚“ãŒç›´æ¥è¡Œã„ã¾ã™ã€‚"

    # æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    emotion = check_emotion(message)
    if emotion:
        if emotion == EscalationLevel.HIGH:
            return (
                "ã€ç·Šæ€¥ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‘\n"
                "å¤§åˆ‡ãªãŠè©±ã‚’ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\n"
                "é‡‘å­ãŒç›´æ¥ãŠè©±ã‚’ä¼ºã„ã¾ã™ã€‚\n\n"
                "ç·Šæ€¥ã®å ´åˆ:\n"
                "ã„ã®ã¡ã®é›»è©±: 0120-783-556"
            )
        elif emotion == EscalationLevel.MID:
            return add_ai_disclosure(
                f"{customer_name}ã•ã‚“ã€ãŠæ°—æŒã¡å—ã‘æ­¢ã‚ã¾ã—ãŸã€‚\n"
                "ã“ã®ã‚ˆã†ãªãŠè©±ã¯é‡‘å­ãŒç›´æ¥ãŠè¿”äº‹ã„ãŸã—ã¾ã™ã€‚\n"
                "48æ™‚é–“ä»¥å†…ã«ã”é€£çµ¡ã—ã¾ã™ã­ã€‚",
                "ç§˜æ›¸ã‚µãƒã‚³"
            )
        else:
            return add_ai_disclosure(
                f"{customer_name}ã•ã‚“ã€ã”ç›¸è«‡ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\n"
                "é‡‘å­ã«å…±æœ‰ã—ã¾ã—ãŸã€‚24æ™‚é–“ä»¥å†…ã«ç¢ºèªã„ãŸã—ã¾ã™ã€‚",
                "ç§˜æ›¸ã‚µãƒã‚³"
            )

    # FAQå¯¾å¿œ
    msg_lower = message.lower()

    if any(kw in msg_lower for kw in ["è¦–è´", "è¦‹æ–¹", "å†ç”Ÿ", "å‹•ç”»"]):
        response = (
            "è¬›åº§ã®è¦–è´æ–¹æ³•ã§ã™ã­ï¼\n"
            "ãƒã‚¤ãƒšãƒ¼ã‚¸ã®ã€Œè¬›åº§ä¸€è¦§ã€ã‹ã‚‰ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚\n"
            "ã”ä¸æ˜ç‚¹ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãï¼"
        )
    elif any(kw in msg_lower for kw in ["èª²é¡Œ", "æå‡º", "ãƒ¯ãƒ¼ã‚¯"]):
        response = (
            "èª²é¡Œã®æå‡ºæ–¹æ³•ã§ã™ã­ï¼\n"
            "ãƒã‚¤ãƒšãƒ¼ã‚¸ã®ã€Œèª²é¡Œæå‡ºã€ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n"
            "æœŸé™ã¯å„é€±ã®æ—¥æ›œ23:59ã¾ã§ã§ã™ã€‚"
        )
    elif any(kw in msg_lower for kw in ["æ–™é‡‘", "ä¾¡æ ¼", "ãƒ—ãƒ©ãƒ³"]):
        response = (
            "ãƒ—ãƒ©ãƒ³ã«ã¤ã„ã¦ã§ã™ã­ï¼\n"
            "è©³ç´°ã¯å…¬å¼ã‚µã‚¤ãƒˆã§ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã€‚\n"
            "ã”è³ªå•ãŒã‚ã‚Œã°é‡‘å­ã«ç›´æ¥ãŠèããã ã•ã„ã­ã€‚"
        )
    else:
        # Central DBã‹ã‚‰FAQãƒŠãƒ¬ãƒƒã‚¸ã‚’æ¤œç´¢
        knowledge_hint = ""
        if use_knowledge:
            faq_results = _search_faq_knowledge(message)
            if faq_results:
                knowledge_hint = f"\n\nã€é–¢é€£ã™ã‚‹è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå‚è€ƒï¼‰ã€‘\n{faq_results}"

        response = (
            f"{customer_name}ã•ã‚“ã€ãŠå•ã„åˆã‚ã›ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\n"
            "ã“ã®å†…å®¹ã¯é‡‘å­ã«ç¢ºèªã—ã¦ã€æ”¹ã‚ã¦ã”é€£çµ¡ã—ã¾ã™ã­ã€‚"
            f"{knowledge_hint}"
        )

    return add_ai_disclosure(response, "ç§˜æ›¸ã‚µãƒã‚³ï¼ˆCentral DBå‚ç…§ï¼‰" if use_knowledge else "ç§˜æ›¸ã‚µãƒã‚³")


# =============================================================================
# ã‚±ãƒ³ã‚¸ï¼ˆãƒªã‚µãƒ¼ãƒï¼‰- Central DBé€£æºç‰ˆ
# =============================================================================

@mcp.tool()
def kenji_research(
    query: str,
    research_type: str = "general",
    category: str = "",
    limit: int = 5,
) -> str:
    """
    ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œã‚±ãƒ³ã‚¸ã€ã«ã‚ˆã‚‹èª¿æŸ»

    ç«¶åˆåˆ†æã€ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ã€Central DBãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ãªã©ã‚’è¡Œã„ã¾ã™ã€‚
    èª¿æŸ»çµæœã¯é‡‘å­ã•ã‚“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµŒã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

    Args:
        query: èª¿æŸ»ã‚¯ã‚¨ãƒª
        research_type: èª¿æŸ»ç¨®åˆ¥ (competitor/trend/knowledge/general)
        category: Central DBæ¤œç´¢æ™‚ã®ã‚«ãƒ†ã‚´ãƒªçµã‚Šè¾¼ã¿ (methods/questions/clients/business/content/tech)
        limit: æ¤œç´¢çµæœã®æœ€å¤§ä»¶æ•°

    Returns:
        èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    if research_type == "knowledge":
        # Central DBã‹ã‚‰RAGæ¤œç´¢
        results = central_db.search_knowledge(
            query=query,
            category=category if category else None,
            limit=limit
        )

        if not results:
            report = f"""# çŸ¥è­˜æ¤œç´¢ãƒ¬ãƒãƒ¼ãƒˆ: {query}

**æ¤œç´¢æ—¥æ™‚**: {timestamp}
**ã‚«ãƒ†ã‚´ãƒª**: {category or "å…¨ã‚«ãƒ†ã‚´ãƒª"}

## æ¤œç´¢çµæœ

Central DBã«è©²å½“ã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

### è€ƒãˆã‚‰ã‚Œã‚‹åŸå› 
- æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦ã¿ã¦ãã ã•ã„
- ã¾ã é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ãŒç™»éŒ²ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""
        else:
            report = f"""# çŸ¥è­˜æ¤œç´¢ãƒ¬ãƒãƒ¼ãƒˆ: {query}

**æ¤œç´¢æ—¥æ™‚**: {timestamp}
**ã‚«ãƒ†ã‚´ãƒª**: {category or "å…¨ã‚«ãƒ†ã‚´ãƒª"}
**ãƒ’ãƒƒãƒˆä»¶æ•°**: {len(results)}ä»¶

## æ¤œç´¢çµæœ

"""
            for i, r in enumerate(results, 1):
                similarity = r.get('similarity', 0)
                confidence = "â˜…â˜…â˜…â˜…â˜…" if similarity > 0.85 else "â˜…â˜…â˜…â˜…â˜†" if similarity > 0.75 else "â˜…â˜…â˜…â˜†â˜†" if similarity > 0.65 else "â˜…â˜…â˜†â˜†â˜†"

                report += f"""### {i}. {r.get('title', 'ç„¡é¡Œ')}
- **ã‚«ãƒ†ã‚´ãƒª**: {r.get('category', '-')} / {r.get('subcategory', '-') or '-'}
- **é–¢é€£åº¦**: {confidence} ({similarity:.1%})
- **ã‚¿ã‚°**: {', '.join(r.get('tags', []) or ['-'])}

{r.get('summary', '') or r.get('content', '')[:300]}{'...' if len(r.get('content', '')) > 300 else ''}

"""

            report += """---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒCentral DBã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ*
*æœ€çµ‚ç¢ºèªã¯é‡‘å­ã•ã‚“ãŒè¡Œã„ã¾ã™*
"""
        return report

    elif research_type == "categories":
        # ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å–å¾—
        categories = central_db.get_categories()

        if not categories:
            return f"""# ã‚«ãƒ†ã‚´ãƒªä¸€è¦§

**å–å¾—æ—¥æ™‚**: {timestamp}

ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""

        report = f"""# Central DB ã‚«ãƒ†ã‚´ãƒªä¸€è¦§

**å–å¾—æ—¥æ™‚**: {timestamp}

## ã‚«ãƒ†ã‚´ãƒª

| ã‚¢ã‚¤ã‚³ãƒ³ | ã‚«ãƒ†ã‚´ãƒª | èª¬æ˜ | ä»¶æ•° |
|:------:|--------|------|-----:|
"""
        for cat_id, info in categories.items():
            report += f"| {info.get('icon', 'ğŸ“')} | **{info.get('name', cat_id)}** ({cat_id}) | {info.get('description', '-')} | {info.get('count', 0)}ä»¶ |\n"

        report += """
---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""
        return report

    elif research_type == "competitor":
        report = f"""# ç«¶åˆèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ: {query}

**èª¿æŸ»æ—¥æ™‚**: {timestamp}
**ä¿¡é ¼åº¦**: â˜…â˜…â˜…â˜†â˜† (è¦è¿½åŠ èª¿æŸ»)

## èª¿æŸ»æ¦‚è¦

ç«¶åˆãƒãƒ£ãƒ³ãƒãƒ«ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ã—ã¾ã—ãŸã€‚

### åŸºæœ¬æƒ…å ±
- ãƒãƒ£ãƒ³ãƒãƒ«å: {query}
- æ¨å®šç™»éŒ²è€…: [è¦èª¿æŸ»]
- ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: [è¦èª¿æŸ»]

### å¼·ã¿ãƒ»å¼±ã¿åˆ†æ
[è©³ç´°èª¿æŸ»ãŒå¿…è¦ã§ã™]

### é‡‘å­ã•ã‚“ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ
[åˆ†æçµæœã‚’è¿½åŠ äºˆå®š]

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
*æœ€çµ‚ç¢ºèªã¯é‡‘å­ã•ã‚“ãŒè¡Œã„ã¾ã™*
"""

    elif research_type == "trend":
        report = f"""# ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ: {query}

**èª¿æŸ»æ—¥æ™‚**: {timestamp}
**ä¿¡é ¼åº¦**: â˜…â˜…â˜…â˜†â˜†

## é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- [ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1]
- [ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2]
- [ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3]

## ãƒˆãƒ¬ãƒ³ãƒ‰å‚¾å‘
ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æã§ã™ã€‚

## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
[ææ¡ˆã‚’è¿½åŠ äºˆå®š]

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""

    else:  # general
        # ä¸€èˆ¬èª¿æŸ»ã§ã‚‚Central DBã‚’æ¤œç´¢ã—ã¦ã¿ã‚‹
        results = central_db.search_knowledge(query=query, limit=3)

        related_section = ""
        if results:
            related_section = "\n## Central DBé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸\n\n"
            for r in results:
                related_section += f"- **{r.get('title', 'ç„¡é¡Œ')}** ({r.get('category', '-')}): {r.get('summary', '')[:100]}...\n"
            related_section += "\nè©³ç´°ã¯ `research_type: knowledge` ã§æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚\n"

        report = f"""# èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ: {query}

**èª¿æŸ»æ—¥æ™‚**: {timestamp}

## èª¿æŸ»å†…å®¹

ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ã—ã¾ã—ãŸã€‚

### æ¦‚è¦
[èª¿æŸ»çµæœã‚’ã“ã“ã«è¨˜è¼‰]

### è©³ç´°
[è©³ç´°åˆ†æã‚’è¿½åŠ äºˆå®š]
{related_section}
---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œã‚±ãƒ³ã‚¸ã€ãŒä½œæˆã—ã¾ã—ãŸ*
*æœ€çµ‚ç¢ºèªã¯é‡‘å­ã•ã‚“ãŒè¡Œã„ã¾ã™*
"""

    return report


# =============================================================================
# ãƒ¦ã‚¦ã‚¿ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼‰- Central DBé€£æºç‰ˆ
# =============================================================================

def _get_knowledge_context(topic: str, content_type: str) -> str:
    """ãƒˆãƒ”ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
    context_parts = []

    # ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»æŠ€æ³•ã‚’æ¤œç´¢
    methods = central_db.search_knowledge(
        query=f"{topic} æ‰‹æ³• ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯",
        category="methods",
        limit=2
    )
    if methods:
        context_parts.append("ã€é–¢é€£ãƒ¡ã‚½ãƒƒãƒ‰ã€‘")
        for m in methods:
            context_parts.append(f"- {m.get('title', '')}: {(m.get('summary', '') or m.get('content', ''))[:150]}")

    # éå»ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ»å°æœ¬ã‚’æ¤œç´¢
    if content_type == "script":
        scripts = central_db.search_knowledge(
            query=f"{topic} å°æœ¬ å‹•ç”»",
            category="content",
            limit=2
        )
        if scripts:
            context_parts.append("\nã€å‚è€ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘")
            for s in scripts:
                context_parts.append(f"- {s.get('title', '')}")

    # è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    questions = central_db.search_knowledge(
        query=topic,
        category="questions",
        limit=2
    )
    if questions:
        context_parts.append("\nã€ã‚ˆãã‚ã‚‹è³ªå•ã€‘")
        for q in questions:
            context_parts.append(f"- {q.get('title', '')}")

    return "\n".join(context_parts) if context_parts else ""


@mcp.tool()
def yuta_create(
    topic: str,
    content_type: str = "script",
    use_knowledge: bool = True,
) -> str:
    """
    ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œãƒ¦ã‚¦ã‚¿ã€ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆ

    å°æœ¬ä¸‹æ›¸ãã€ãƒ•ãƒƒã‚¯æ–‡æ¡ˆã€ãƒ¡ãƒ«ãƒã‚¬ä¸‹æ›¸ããªã©ã‚’ä½œæˆã—ã¾ã™ã€‚
    Central DBã‹ã‚‰é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã‚’å–å¾—ã—ã¦å‚è€ƒæƒ…å ±ã¨ã—ã¦æç¤ºã—ã¾ã™ã€‚
    HSPé…æ…®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã„ã€æœ€çµ‚ç‰ˆã¯é‡‘å­ã•ã‚“ãŒå®Œæˆã•ã›ã¾ã™ã€‚

    Args:
        topic: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ”ãƒƒã‚¯
        content_type: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®åˆ¥ (script/hook/newsletter/thumbnail)
        use_knowledge: Central DBã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚’å–å¾—ã™ã‚‹ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)

    Returns:
        ä½œæˆã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    # Central DBã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    knowledge_context = ""
    if use_knowledge:
        knowledge_context = _get_knowledge_context(topic, content_type)

    # ãƒŠãƒ¬ãƒƒã‚¸ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    knowledge_section = ""
    if knowledge_context:
        knowledge_section = f"""
---

## Central DBã‹ã‚‰ã®å‚è€ƒæƒ…å ±

{knowledge_context}

"""

    if content_type == "hook":
        content = f"""# ãƒ•ãƒƒã‚¯æ–‡æ¡ˆ: {topic}

**ä½œæˆæ—¥æ™‚**: {timestamp}
**ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§**: {"ã‚ã‚Š" if knowledge_context else "ãªã—"}

## å¥½å¥‡å¿ƒå‹
1. å®Ÿã¯ã€{topic}ã«ã¯èª°ã‚‚æ•™ãˆã¦ãã‚Œãªã„ã€Œéš ã•ã‚ŒãŸç†ç”±ã€ãŒã‚ã‚Šã¾ã™
2. {topic}ã‚’æ­£ã—ãç†è§£ã—ã¦ã„ã‚‹äººã¯ã€å®Ÿã¯10%ã‚‚ã„ã¾ã›ã‚“
3. ç§ãŒ{topic}ã«ã¤ã„ã¦æ°—ã¥ã„ãŸã€æ„å¤–ãªçœŸå®Ÿã‚’ãŠè©±ã—ã—ã¾ã™

## å…±æ„Ÿå‹ï¼ˆHSPå‘ã‘æ¨å¥¨ï¼‰
1. ã€Œ{topic}ã€ã§æ‚©ã‚“ã§ã„ã¾ã›ã‚“ã‹ï¼Ÿã‚ãªãŸã ã‘ã˜ã‚ƒãªã„ã‚“ã§ã™
2. ã‚‚ã—{topic}ãŒã†ã¾ãã„ã‹ãªã„ã¨æ„Ÿã˜ã¦ã„ã‚‹ãªã‚‰ã€ã“ã®å‹•ç”»ãŒå½¹ã«ç«‹ã¤ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
3. ç§ã‚‚ä»¥å‰ã€{topic}ã§æœ¬å½“ã«è‹¦ã—ã‚“ã§ã„ã¾ã—ãŸ

## å•é¡Œæèµ·å‹
1. ãªãœã‚ãªãŸã®{topic}ã¯ã†ã¾ãã„ã‹ãªã„ã®ã‹ï¼Ÿ3ã¤ã®åŸå› 
2. {topic}ã§å¤±æ•—ã™ã‚‹äººã«å…±é€šã™ã‚‹ã€ãŸã£ãŸ1ã¤ã®ç‰¹å¾´
3. ã€Œ{topic}ã€ã‚’è«¦ã‚ã‚‹å‰ã«ã€ã“ã‚Œã ã‘ã¯çŸ¥ã£ã¦ãŠã„ã¦ãã ã•ã„
{knowledge_section}
---
**æ¨å¥¨**: HSPå‘ã‘ã«ã¯ã€Œå…±æ„Ÿå‹ã€ãŒåŠ¹æœçš„ã§ã™

*ã“ã®æ¡ˆã¯AIã€Œãƒ¦ã‚¦ã‚¿ã€ãŒä½œæˆã—ã¾ã—ãŸï¼ˆCentral DBå‚ç…§ï¼‰*
*æœ€çµ‚æ±ºå®šã¯é‡‘å­ã•ã‚“ã§*
"""

    elif content_type == "thumbnail":
        content = f"""# ã‚µãƒ ãƒã‚¤ãƒ«ã‚³ãƒ”ãƒ¼æ¡ˆ: {topic}

**ä½œæˆæ—¥æ™‚**: {timestamp}
**ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§**: {"ã‚ã‚Š" if knowledge_context else "ãªã—"}

## ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–
1. çŸ¥ã‚‰ãªã„ã¨æã™ã‚‹{topic}ã®çœŸå®Ÿ
2. {topic}ã§æ‚©ã‚€äººã¸

## å…±æ„Ÿé‡è¦–ï¼ˆHSPå‘ã‘æ¨å¥¨ï¼‰
3. ç§ãŒ{topic}ã§å­¦ã‚“ã ã“ã¨
4. HSPã•ã‚“ã®ãŸã‚ã®{topic}è¬›åº§

## ã‚·ãƒ³ãƒ—ãƒ«
5. ã“ã‚Œã ã‘ã¯çŸ¥ã£ã¦ãŠã„ã¦ï½œ{topic}
6. {topic}ã®ã€Œæœ¬å½“ã®æ„å‘³ã€
{knowledge_section}
---
**ãƒ‡ã‚¶ã‚¤ãƒ³æ¨å¥¨**:
- èƒŒæ™¯: è½ã¡ç€ã„ãŸè‰²èª¿ï¼ˆé’ãƒ»ç·‘ç³»ï¼‰
- ãƒ•ã‚©ãƒ³ãƒˆ: èª­ã¿ã‚„ã™ã„å¤ªã‚ã®ã‚´ã‚·ãƒƒã‚¯
- æ–‡å­—æ•°: 15æ–‡å­—ä»¥å†…ãŒç†æƒ³

*ã“ã®æ¡ˆã¯AIã€Œãƒ¦ã‚¦ã‚¿ã€ãŒä½œæˆã—ã¾ã—ãŸï¼ˆCentral DBå‚ç…§ï¼‰*
"""

    elif content_type == "newsletter":
        content = f"""# ãƒ¡ãƒ«ãƒã‚¬ä¸‹æ›¸ã: {topic}

**ä½œæˆæ—¥æ™‚**: {timestamp}
**ãƒŠãƒ¬ãƒƒã‚¸å‚ç…§**: {"ã‚ã‚Š" if knowledge_context else "ãªã—"}

## ä»¶åæ¡ˆï¼ˆ3æ¡ˆï¼‰
1. ã€Œ{topic}ã€ã«ã¤ã„ã¦ã€ã¡ã‚‡ã£ã¨ã ã‘ãŠè©±ã—ã•ã›ã¦ãã ã•ã„
2. ã€é‡‘å­ã‚ˆã‚Šã€‘{topic}ã§å¤§åˆ‡ãªã“ã¨
3. ã”è³ªå•ã„ãŸã ã„ãŸã€Œ{topic}ã€ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™

---

## æœ¬æ–‡

ã“ã‚“ã«ã¡ã¯ã€é‡‘å­ã§ã™ã€‚

ä»Šæ—¥ã¯ã€Œ{topic}ã€ã«ã¤ã„ã¦
å°‘ã—ã ã‘ãŠè©±ã—ã•ã›ã¦ãã ã•ã„ã€‚

ã€ã“ã“ã«é‡‘å­ã•ã‚“ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã€‘

---

ã‚‚ã—ã‚ãªãŸã‚‚åŒã˜ã‚ˆã†ã«æ„Ÿã˜ã¦ã„ãŸã‚‰ã€
ã“ã®è€ƒãˆæ–¹ãŒå°‘ã—ã§ã‚‚å‚è€ƒã«ãªã‚Œã°å¬‰ã—ã„ã§ã™ã€‚

ç„¡ç†ã®ãªã„ç¯„å›²ã§ã€
ã”è‡ªèº«ã®ãƒšãƒ¼ã‚¹ã§é€²ã‚“ã§ãã ã•ã„ã­ã€‚

ãã‚Œã§ã¯ã€ã¾ãŸã€‚

é‡‘å­
{knowledge_section}
---
*ã“ã®ä¸‹æ›¸ãã¯AIã€Œãƒ¦ã‚¦ã‚¿ã€ãŒä½œæˆã—ã¾ã—ãŸï¼ˆCentral DBå‚ç…§ï¼‰*
*æœ€çµ‚ç‰ˆã¯é‡‘å­ã•ã‚“ãŒå®Œæˆã•ã›ã¾ã™*
"""

    else:  # script
        content = f"""# å°æœ¬ä¸‹æ›¸ã: {topic}

**ä½œæˆæ—¥æ™‚**: {timestamp}
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ä¸‹æ›¸ãï¼ˆé‡‘å­ã•ã‚“ã®ç¢ºèªãŒå¿…è¦ï¼‰

---

## ãƒ•ãƒƒã‚¯ï¼ˆå°å…¥15ç§’ï¼‰

ã€3æ¡ˆã‹ã‚‰é¸æŠã€‘
1. ã€Œ{topic}ã€ã§æ‚©ã‚“ã§ã„ã¾ã›ã‚“ã‹ï¼Ÿ
2. å®Ÿã¯ã€{topic}ã«ã¯éš ã•ã‚ŒãŸç†ç”±ãŒã‚ã‚Šã¾ã™
3. ç§ãŒ{topic}ã§æ°—ã¥ã„ãŸå¤§åˆ‡ãªã“ã¨

---

## å°å…¥éƒ¨ï¼ˆ30ç§’ï¼‰

ã“ã‚“ã«ã¡ã¯ã€é‡‘å­ã§ã™ã€‚
ä»Šæ—¥ã¯ã€Œ{topic}ã€ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã™ã€‚

ã€ã“ã“ã«é‡‘å­ã•ã‚“ã®å®Ÿä½“é¨“ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‘
ä¾‹: ã€Œå®Ÿã¯ç§ã‚‚ä»¥å‰ã€ã€‡ã€‡ã§æ‚©ã‚“ã§ã„ãŸæ™‚æœŸãŒã‚ã‚Šã¾ã—ãŸ...ã€

---

## æœ¬ç·¨

### ç¬¬1ç« : ã€‡ã€‡ã¨ã¯ï¼ˆ2åˆ†ï¼‰
[ãƒã‚¤ãƒ³ãƒˆã‚’èª¬æ˜]

ã€é‡‘å­ã•ã‚“ã®è£œè¶³ãƒã‚¤ãƒ³ãƒˆã€‘
_ã“ã“ã«é‡‘å­ã•ã‚“ãªã‚‰ã§ã¯ã®è¦–ç‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„_

### ç¬¬2ç« : ãªãœå¤§åˆ‡ãªã®ã‹ï¼ˆ3åˆ†ï¼‰
[å…·ä½“ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜]

ã€é‡‘å­ã•ã‚“ã®å®Ÿä½“é¨“ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€‘
_ã“ã“ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆäº‹ä¾‹ï¼ˆåŒ¿ååŒ–ï¼‰ã‚’å…¥ã‚Œã¦ãã ã•ã„_

### ç¬¬3ç« : å®Ÿè·µã®ãƒ’ãƒ³ãƒˆï¼ˆ3åˆ†ï¼‰

ã”è‡ªèº«ã®ãƒšãƒ¼ã‚¹ã§ã€ç„¡ç†ã®ãªã„ç¯„å›²ã§
è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

---

## ã¾ã¨ã‚ï¼ˆ1åˆ†ï¼‰

ä»Šæ—¥ã¯ã€Œ{topic}ã€ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã—ãŸã€‚

ã‚‚ã—å‚è€ƒã«ãªã£ãŸã‚‰ã€
ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²ã—ã¦ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚

---

## ç¢ºèªäº‹é …ï¼ˆé‡‘å­ã•ã‚“ã¸ï¼‰
1. ãƒ•ãƒƒã‚¯æ¡ˆã®ã©ã‚Œã‚’æ¡ç”¨ã—ã¾ã™ã‹ï¼Ÿ
2. å®Ÿä½“é¨“ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
3. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆäº‹ä¾‹ã‚’1ã¤å…¥ã‚Œã¦ãã ã•ã„
{knowledge_section}
---
*ã“ã®ä¸‹æ›¸ãã¯AIã€Œãƒ¦ã‚¦ã‚¿ã€ãŒä½œæˆã—ã¾ã—ãŸï¼ˆCentral DBå‚ç…§ï¼‰*
*æœ€çµ‚ç‰ˆã¯é‡‘å­ã•ã‚“ãŒå®Œæˆã•ã›ã¾ã™*
"""

    return content


# =============================================================================
# ãƒã‚³ãƒˆï¼ˆå“è³ªï¼†å€«ç†ï¼‰- Central DBé€£æºç‰ˆ
# =============================================================================

def _get_quality_guidelines(content: str) -> str:
    """Central DBã‹ã‚‰å“è³ªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—"""
    guidelines = []

    # methodsã‚«ãƒ†ã‚´ãƒªã‹ã‚‰HSPé–¢é€£ã®æ‰‹æ³•ã‚’æ¤œç´¢
    methods = central_db.search_knowledge(
        query="HSP é…æ…® ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
        category="methods",
        limit=2
    )
    if methods:
        guidelines.append("ã€HSPé…æ…®ã®ãƒ¡ã‚½ãƒƒãƒ‰ã€‘")
        for m in methods:
            guidelines.append(f"ãƒ»{m.get('title', '')}")

    # contentã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’æ¤œç´¢
    best_practices = central_db.search_knowledge(
        query="å“è³ª ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ è¡¨ç¾",
        category="content",
        limit=2
    )
    if best_practices:
        guidelines.append("\nã€å‚è€ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘")
        for b in best_practices:
            guidelines.append(f"ãƒ»{b.get('title', '')}")

    return "\n".join(guidelines) if guidelines else ""


@mcp.tool()
def makoto_check(
    content: str,
    check_types: str = "all",
    use_knowledge: bool = True,
) -> str:
    """
    å“è³ªï¼†å€«ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œãƒã‚³ãƒˆã€ã«ã‚ˆã‚‹å“è³ªãƒã‚§ãƒƒã‚¯

    HSPå…±æ„Ÿåº¦ã€å€«ç†ã€æŠ€è¡“å“è³ªã€é€æ˜æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚
    Central DBã‹ã‚‰HSPé…æ…®ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã—ã¦å‚ç…§ã—ã¾ã™ã€‚
    å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ä¿®æ­£ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚

    Args:
        content: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        check_types: ãƒã‚§ãƒƒã‚¯ç¨®åˆ¥ (hsp/ethics/technical/transparency/all)
        use_knowledge: Central DBã‹ã‚‰ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ã™ã‚‹ã‹

    Returns:
        ãƒã‚§ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆ
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    issues = []
    suggestions = []
    scores = {}

    # HSPãƒã‚§ãƒƒã‚¯
    if check_types in ["hsp", "all"]:
        hsp_issues = []
        for expr in HSP_AVOID:
            if expr in content:
                hsp_issues.append(f"ã€Œ{expr}ã€ã¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

        hsp_score = max(1, 10 - len(hsp_issues) * 2)
        scores["HSPå…±æ„Ÿåº¦"] = hsp_score

        if hsp_issues:
            issues.extend(hsp_issues)
            suggestions.append("å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹è¡¨ç¾ï¼ˆã€Œã”è‡ªèº«ã®ãƒšãƒ¼ã‚¹ã§ã€ã€Œç„¡ç†ã®ãªã„ç¯„å›²ã§ã€ãªã©ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

    # å€«ç†ãƒã‚§ãƒƒã‚¯
    if check_types in ["ethics", "all"]:
        ethics_issues = []

        # å±é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        danger_keywords = ["è‡ªæ®º", "è‡ªå‚·", "æ­»ã«ãŸã„"]
        for kw in danger_keywords:
            if kw in content:
                ethics_issues.append(f"å±é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: ã€Œ{kw}ã€")

        # è¦æ³¨æ„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        warning_keywords = ["åŒ»ç™‚", "è¨ºæ–­", "æ²»ç™‚", "æ³•å¾‹", "è¨´è¨Ÿ"]
        for kw in warning_keywords:
            if kw in content:
                ethics_issues.append(f"è¦æ³¨æ„: ã€Œ{kw}ã€- å°‚é–€å®¶ã¸ã®ç›¸è«‡ã‚’ä¿ƒã—ã¦ãã ã•ã„")

        ethics_score = 1 if any("å±é™º" in i for i in ethics_issues) else max(1, 10 - len(ethics_issues) * 2)
        scores["å€«ç†"] = ethics_score

        if ethics_issues:
            issues.extend(ethics_issues)

    # æŠ€è¡“å“è³ªãƒã‚§ãƒƒã‚¯
    if check_types in ["technical", "all"]:
        tech_score = 10

        if len(content) < 50:
            issues.append("å†…å®¹ãŒçŸ­ã™ãã¾ã™")
            tech_score -= 2

        if len(content.split("\n\n")) < 2:
            suggestions.append("æ®µè½åˆ†ã‘ã‚’è¿½åŠ ã™ã‚‹ã¨èª­ã¿ã‚„ã™ããªã‚Šã¾ã™")
            tech_score -= 1

        scores["æŠ€è¡“å“è³ª"] = max(1, tech_score)

    # é€æ˜æ€§ãƒã‚§ãƒƒã‚¯
    if check_types in ["transparency", "all"]:
        has_disclosure = any(pattern in content for pattern in
            ["AIã§ã™", "AIã€Œ", "è‡ªå‹•è¿”ä¿¡", "AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "ãŒä½œæˆã—ã¾ã—ãŸ"])

        if not has_disclosure:
            issues.append("AIé–‹ç¤ºãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            suggestions.append("ã€Œã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯AIè‡ªå‹•è¿”ä¿¡ã§ã™ã€ãªã©ã®é–‹ç¤ºã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

        scores["é€æ˜æ€§"] = 10 if has_disclosure else 3

    # ç·åˆåˆ¤å®š
    overall_score = sum(scores.values()) / len(scores) if scores else 0
    overall_passed = all(s >= 7 for s in scores.values()) and not any("å±é™º" in i for i in issues)

    status = "âœ… æ‰¿èªæ¨å¥¨" if overall_passed else "âš ï¸ è¦ä¿®æ­£"

    report = f"""# å“è³ªãƒã‚§ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆ

**ç·åˆåˆ¤å®š**: {status}
**ç·åˆã‚¹ã‚³ã‚¢**: {overall_score:.1f}/10
**ãƒã‚§ãƒƒã‚¯æ—¥æ™‚**: {timestamp}

---

## ã‚¹ã‚³ã‚¢è©³ç´°

"""
    for check_name, score in scores.items():
        icon = "âœ…" if score >= 7 else "âŒ"
        report += f"- {check_name}: {score}/10 {icon}\n"

    if issues:
        report += "\n## å•é¡Œç‚¹\n\n"
        for issue in issues:
            report += f"- {issue}\n"

    if suggestions:
        report += "\n## æ”¹å–„ææ¡ˆ\n\n"
        for suggestion in suggestions:
            report += f"- {suggestion}\n"

    # Central DBã‹ã‚‰ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—
    if use_knowledge:
        guidelines = _get_quality_guidelines(content)
        if guidelines:
            report += f"\n## Central DBã‹ã‚‰ã®å‚è€ƒã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³\n\n{guidelines}\n"

    db_note = "ï¼ˆCentral DBå‚ç…§ï¼‰" if use_knowledge else ""
    report += f"""
---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€Œãƒã‚³ãƒˆã€ãŒä½œæˆã—ã¾ã—ãŸ{db_note}*
*æœ€çµ‚åˆ¤æ–­ã¯é‡‘å­ã•ã‚“ãŒè¡Œã„ã¾ã™*
"""

    return report


# =============================================================================
# ãƒŠã‚ªãƒŸï¼ˆå­¦ç¿’ï¼†åˆ†æï¼‰- Central DBé€£æºç‰ˆ
# =============================================================================

def _get_analysis_context(analysis_type: str, query: str = "") -> str:
    """Central DBã‹ã‚‰åˆ†æã«å½¹ç«‹ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
    context_parts = []

    if analysis_type == "progress":
        # é¡§å®¢è‚²æˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œç´¢
        methods = central_db.search_knowledge(
            query="é¡§å®¢è‚²æˆ ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ— ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ",
            category="methods",
            limit=2
        )
        if methods:
            context_parts.append("ã€é¡§å®¢è‚²æˆãƒ¡ã‚½ãƒƒãƒ‰ã€‘")
            for m in methods:
                context_parts.append(f"ãƒ»{m.get('title', '')}")

    elif analysis_type == "video":
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã®çŸ¥è¦‹ã‚’æ¤œç´¢
        content_insights = central_db.search_knowledge(
            query="å‹•ç”» ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ åˆ†æ",
            category="content",
            limit=2
        )
        if content_insights:
            context_parts.append("ã€å‚è€ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘")
            for c in content_insights:
                context_parts.append(f"ãƒ»{c.get('title', '')}")

    elif analysis_type == "churn":
        # é›¢è„±é˜²æ­¢ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œç´¢
        churn_methods = central_db.search_knowledge(
            query="é›¢è„± é˜²æ­¢ ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³",
            category="methods",
            limit=2
        )
        if churn_methods:
            context_parts.append("ã€é›¢è„±é˜²æ­¢ãƒ¡ã‚½ãƒƒãƒ‰ã€‘")
            for m in churn_methods:
                context_parts.append(f"ãƒ»{m.get('title', '')}")

    # ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æ¤œç´¢
    if query:
        business = central_db.search_knowledge(
            query=query,
            category="business",
            limit=2
        )
        if business:
            context_parts.append("\nã€ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘")
            for b in business:
                context_parts.append(f"ãƒ»{b.get('title', '')}")

    return "\n".join(context_parts) if context_parts else ""


@mcp.tool()
def naomi_analyze(
    analysis_type: str = "progress",
    data: str = "{}",
    use_knowledge: bool = True,
) -> str:
    """
    å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ŒãƒŠã‚ªãƒŸã€ã«ã‚ˆã‚‹åˆ†æ

    å‹•ç”»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€é¡§å®¢é€²æ—ã€é›¢è„±ãƒªã‚¹ã‚¯ãªã©ã‚’åˆ†æã—ã¾ã™ã€‚
    Central DBã‹ã‚‰é–¢é€£ã™ã‚‹åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ãƒ»ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å‚ç…§ã—ã¾ã™ã€‚
    Premiumãƒ‡ãƒ¼ã‚¿ã®åˆ†æã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã€‚

    Args:
        analysis_type: åˆ†æç¨®åˆ¥ (video/progress/churn/monthly/knowledge)
        data: åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ (JSONæ–‡å­—åˆ—)
        use_knowledge: Central DBã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚’å–å¾—ã™ã‚‹ã‹

    Returns:
        åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    try:
        parsed_data = json.loads(data) if data != "{}" else {}
    except json.JSONDecodeError:
        parsed_data = {}

    # Premiumé¡§å®¢ãƒã‚§ãƒƒã‚¯
    if parsed_data.get("tier") == "premium":
        return "ã€ãƒ–ãƒ­ãƒƒã‚¯ã€‘Premiumé¡§å®¢ã®åˆ†æã¯é‡‘å­ã•ã‚“ãŒç›´æ¥è¡Œã„ã¾ã™ã€‚"

    if analysis_type == "video":
        report = f"""# å‹•ç”»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

**åˆ†ææ—¥æ™‚**: {timestamp}

## ã‚µãƒãƒªãƒ¼

- **å¯¾è±¡å‹•ç”»æ•°**: {parsed_data.get('video_count', 'ä¸æ˜')}æœ¬
- **ç·è¦–è´å›æ•°**: [ãƒ‡ãƒ¼ã‚¿é€£æºå¾Œã«è¡¨ç¤º]
- **å¹³å‡CTR**: [ãƒ‡ãƒ¼ã‚¿é€£æºå¾Œã«è¡¨ç¤º]

## ã‚¤ãƒ³ã‚µã‚¤ãƒˆ

1. [åˆ†æçµæœã‚’è¿½åŠ äºˆå®š]
2. [ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºäºˆå®š]

## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

- [æ”¹å–„ææ¡ˆã‚’è¿½åŠ äºˆå®š]

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€ŒãƒŠã‚ªãƒŸã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""

    elif analysis_type == "progress":
        customer_name = parsed_data.get("customer_name", "ãŠå®¢æ§˜")
        completion = parsed_data.get("completion_rate", 0)
        days_since = parsed_data.get("days_since_login", 0)

        risk = "é«˜" if days_since >= 14 else "ä¸­" if days_since >= 7 else "ä½"

        report = f"""# é¡§å®¢é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ

**åˆ†ææ—¥æ™‚**: {timestamp}

## {customer_name}ã•ã‚“ã®é€²æ—

- **è¬›åº§å®Œäº†ç‡**: {completion}%
- **æœ€çµ‚ãƒ­ã‚°ã‚¤ãƒ³**: {days_since}æ—¥å‰
- **é›¢è„±ãƒªã‚¹ã‚¯**: {risk}

## ã‚¤ãƒ³ã‚µã‚¤ãƒˆ

"""
        if days_since >= 14:
            report += "- âš ï¸ 14æ—¥ä»¥ä¸Šãƒ­ã‚°ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n"
        elif completion >= 80:
            report += "- è¬›åº§é€²æ—ãŒé †èª¿ã§ã™ï¼\n"
        elif completion < 30:
            report += "- è¬›åº§é€²æ—ãŒåœæ»ã—ã¦ã„ã¾ã™ã€‚åŠ±ã¾ã—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"

        report += """
## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

"""
        if days_since >= 14:
            report += "- ã€ç·Šæ€¥ã€‘é‡‘å­ã•ã‚“ã‹ã‚‰ã®ç›´æ¥ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨\n"
        elif days_since >= 7:
            report += "- åŠ±ã¾ã—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é€ä¿¡ã‚’æ¤œè¨\n"

        report += """
---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€ŒãƒŠã‚ªãƒŸã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""

    elif analysis_type == "churn":
        report = f"""# é›¢è„±ãƒªã‚¹ã‚¯æ¤œçŸ¥ãƒ¬ãƒãƒ¼ãƒˆ

**æ¤œçŸ¥æ—¥æ™‚**: {timestamp}

## é«˜ãƒªã‚¹ã‚¯é¡§å®¢ï¼ˆ14æ—¥ä»¥ä¸Šãƒ­ã‚°ã‚¤ãƒ³ãªã—ï¼‰

[é¡§å®¢ãƒ‡ãƒ¼ã‚¿é€£æºå¾Œã«è¡¨ç¤º]

## ä¸­ãƒªã‚¹ã‚¯é¡§å®¢ï¼ˆ7-13æ—¥ãƒ­ã‚°ã‚¤ãƒ³ãªã—ï¼‰

[é¡§å®¢ãƒ‡ãƒ¼ã‚¿é€£æºå¾Œã«è¡¨ç¤º]

## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

- é«˜ãƒªã‚¹ã‚¯é¡§å®¢ã«ã¯é‡‘å­ã•ã‚“ã‹ã‚‰ã®ç›´æ¥ãƒ•ã‚©ãƒ­ãƒ¼ã‚’
- ä¸­ãƒªã‚¹ã‚¯é¡§å®¢ã«ã¯åŠ±ã¾ã—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯AIã€ŒãƒŠã‚ªãƒŸã€ãŒä½œæˆã—ã¾ã—ãŸ*
"""

    else:  # monthly
        period = parsed_data.get("period", datetime.now().strftime("%Yå¹´%mæœˆ"))

        report = f"""# æœˆæ¬¡ãƒ¬ãƒãƒ¼ãƒˆ: {period}

**ä½œæˆæ—¥**: {timestamp}

## KPIã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | æ•°å€¤ | å‰æœˆæ¯” |
|-----|------|--------|
| æ–°è¦ä¼šå“¡ | - | - |
| é€€ä¼š | - | - |
| ç¶™ç¶šç‡ | - | - |

## AIæ´»ç”¨çŠ¶æ³

- **AIå¯¾å¿œä»¶æ•°**: [é›†è¨ˆäºˆå®š]
- **äººé–“å¯¾å¿œä»¶æ•°**: [é›†è¨ˆäºˆå®š]

## ä»Šæœˆã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ

- [æˆåŠŸäº‹ä¾‹ã‚’ã“ã“ã«]

## æ¥æœˆã®æ³¨åŠ›ãƒã‚¤ãƒ³ãƒˆ

- [é‡‘å­ã•ã‚“ã«ç¢ºèªã—ã¦è¨˜å…¥]

---
*ã“ã®ä¸‹æ›¸ãã¯AIã€ŒãƒŠã‚ªãƒŸã€ãŒä½œæˆã—ã¾ã—ãŸ*
*æœ€çµ‚ç‰ˆã¯é‡‘å­ã•ã‚“ãŒç¢ºèªãƒ»ç·¨é›†ã—ã¾ã™*
"""

    # Central DBã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    if use_knowledge:
        query = parsed_data.get("customer_name", "") or parsed_data.get("topic", "")
        context = _get_analysis_context(analysis_type, query)
        if context:
            report += f"\n## Central DBã‹ã‚‰ã®å‚è€ƒæƒ…å ±\n\n{context}\n"
        report = report.replace("ãŒä½œæˆã—ã¾ã—ãŸ", "ãŒä½œæˆã—ã¾ã—ãŸï¼ˆCentral DBå‚ç…§ï¼‰")

    return report


# =============================================================================
# ãƒ¡ã‚¤ãƒ³
# =============================================================================

if __name__ == "__main__":
    mcp.run()
